{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b5889a-cde0-4e70-ae94-35b43a36ecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, EncoderDecoderModel, BertGenerationEncoder, BertGenerationConfig, BertGenerationDecoder, BertGenerationTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset,concatenate_datasets\n",
    "\n",
    "autoencoder_checkpoint = \"../checkpoints/latent_space_representation_edit_model_wang_controllable_2019/autoencoder_3500_0.008632369404399974.pth\"\n",
    "classifier_checkpoint = \"../checkpoints/latent_space_representation_edit_model_wang_controllable_2019/classifier_3500_0.04627384876884106.pth\"\n",
    "MAX_SEQUENCE_LENGTH = 60\n",
    "batch_size = 2\n",
    "skip_special_tokens = True\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MASKED = 0\n",
    "NOT_MASKED = 1\n",
    "LEGAL_TEXT_LABEL = 1\n",
    "NON_LEGAL_TEXT_LABEL = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648bc6b0-9e47-4526-aae0-0b9ab4cb1831",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981652f0-b730-40ad-a04b-0b122c434018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Based on the code from @wang_controllable_2019\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(latent_size, 100)\n",
    "        self.relu1 = nn.LeakyReLU(\n",
    "            0.2,\n",
    "        )\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.relu2 = nn.LeakyReLU(0.2)\n",
    "        self.fc3 = nn.Linear(50, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        out = self.fc1(input)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out  # batch_size * label_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69e13ef-e246-4232-9758-9de49d5bf8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = torch.load(classifier_checkpoint, map_location=torch.device(device))\n",
    "autoencoder = torch.load(autoencoder_checkpoint, map_location=torch.device(device))\n",
    "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7db5bd-e85b-4d3a-9d95-647d553a9c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = autoencoder.config.max_length\n",
    "MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22efb609-4c10-460b-b53e-caf5de8ab0cc",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd603ee-c95a-48e1-8c79-782afa041b62",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(tokenizer, max_sequence_length, num_proc=10):\n",
    "    assert tokenizer != None                                              \n",
    "    legal_text = load_dataset(                                        \n",
    "        \"pierreguillou/lener_br_finetuning_language_model\", streaming=False             \n",
    "    )                                                                                 \n",
    "    legal_text = legal_text.map(                                                \n",
    "        lambda x: {\"label\": [LEGAL_TEXT_LABEL] * len(x[\"text\"])}, num_proc=num_proc, batched=True\n",
    "    )   \n",
    "\n",
    "    \n",
    "    \n",
    "    wikipedia = load_dataset(\"jvanz/portuguese_wikipedia_sentences\")                           \n",
    "    wikipedia = wikipedia.map(                                        \n",
    "        lambda x: {\"label\": [NON_LEGAL_TEXT_LABEL] * len(x[\"text\"])}, num_proc=num_proc, batched=True\n",
    "    )                                                                         \n",
    "        \n",
    "    train_dataset = concatenate_datasets([wikipedia[\"train\"], legal_text[\"train\"]])\n",
    "    evaluation_dataset = concatenate_datasets(                              \n",
    "        [wikipedia[\"evaluation\"], legal_text[\"validation\"]]\n",
    "    )                                                                                                                 \n",
    "    legal_text[\"train\"] = train_dataset                   \n",
    "    legal_text[\"evaluation\"] = evaluation_dataset \n",
    "    \n",
    "    legal_text = legal_text.map(                                         \n",
    "        lambda x: tokenizer(                                \n",
    "            x[\"text\"],                                                \n",
    "            add_special_tokens=False,                     \n",
    "            #padding=\"max_length\",                                                       \n",
    "            #truncation=True,                     \n",
    "            #max_length=MAX_SEQUENCE_LENGTH,\n",
    "        ),\n",
    "        num_proc=num_proc,\n",
    "        batched=True,\n",
    "    )\n",
    "    legal_text = legal_text.map(lambda x: {\"tokens_count\": len(x[\"input_ids\"])})\n",
    "\n",
    "    legal_text.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"is_legal\", \"tokens_count\"],\n",
    "    )\n",
    "    legal_text = legal_text.shuffle(seed=42)\n",
    "    print(legal_text)\n",
    "    return legal_text\n",
    "\n",
    "\n",
    "datasets = prepare_dataset(tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "legal_dataset = datasets[\"evaluation\"].filter(lambda x: x[\"label\"] == 1).filter(lambda x: x[\"tokens_count\"] <= 65 and x[\"tokens_count\"] >=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9164b65f-7a22-40c3-bfce-bf0c21f8e02d",
   "metadata": {},
   "source": [
    "# Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5e2a18-c420-419b-a870-5065fb7c6b7b",
   "metadata": {},
   "source": [
    "```\n",
    "@inproceedings{DBLP:journals/corr/abs-1905-12926,\n",
    "  author    = {Ke Wang and Hang Hua and Xiaojun Wan},\n",
    "  title     = {Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation},\n",
    "  booktitle = {NeurIPS},\n",
    "  year      = {2019}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79ac932-76b4-4aba-ab44-f30ae678240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Infere-se dos elementos coligidos que o agravante encontra-se na iminência de sofrer nova sanção, em decorrência do não pagamento da multa aplicada devido à retirada do tamponamento no hidrômetro, por um dos condôminos.\"\n",
    "#sentence = \"Esse é uma sentença para testar o modelo e ver se ele consegue recriar o texto. Ou seja, ver se a autodecodificação funciona.\"\n",
    "tokenizer_output = tokenizer(sentence, \n",
    "    add_special_tokens=False, \n",
    "    padding=\"max_length\", \n",
    "    truncation=True,\n",
    "    max_length=MAX_SEQUENCE_LENGTH,\n",
    "    return_tensors=\"pt\")\n",
    "\n",
    "print(tokenizer_output)\n",
    "outputs = autoencoder.generate(tokenizer_output.input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee84d376-181f-4969-901c-9adbfb27e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.eval()\n",
    "classifier.eval()\n",
    "\n",
    "autoencoder_output = autoencoder(input_ids=tokenizer_output.input_ids, attention_mask=tokenizer_output.attention_mask, labels=tokenizer_output.input_ids, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "data = autoencoder_output.encoder_last_hidden_state.detach()\n",
    "encoder_outputs = transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions(\n",
    "    last_hidden_state=data, \n",
    "    attentions=autoencoder_output.encoder_attentions\n",
    ")\n",
    "output = autoencoder.generate(encoder_outputs=encoder_outputs)\n",
    "print(tokenizer.batch_decode(output, skip_special_tokens=True))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d50d42b-a426-482b-ae9f-b3c7bcadd74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_output = autoencoder(input_ids=tokenizer_output.input_ids, attention_mask=tokenizer_output.attention_mask, labels=tokenizer_output.input_ids, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "data = autoencoder_output.encoder_last_hidden_state.detach() * torch.rand(autoencoder_output.encoder_last_hidden_state.size())\n",
    "encoder_outputs = transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions(\n",
    "    last_hidden_state=data, \n",
    "    attentions=autoencoder_output.encoder_attentions\n",
    ")\n",
    "output = autoencoder.generate(encoder_outputs=encoder_outputs)\n",
    "print(tokenizer.batch_decode(output, skip_special_tokens=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8958b472-fc37-4e00-96d7-5c7f2f9a6f77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fast_gradient_iterative_modification(autoencoder, classifier, autoencoder_output, target, tokenizer):\n",
    "    \"\"\"\n",
    "    Fast Gradient Iterative Methods\n",
    "    \n",
    "    Based on the code from @wang_controllable_2019\n",
    "    \"\"\"\n",
    "    classifier_loss = nn.BCELoss(reduction='mean') \n",
    "    for epsilon in [20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0]:\n",
    "        data = autoencoder_output.encoder_last_hidden_state.clone()\n",
    "        for _ in range(1, 5):\n",
    "            classifier.zero_grad()                                                                                                                                                                                                                \n",
    "            print(\"epsilon:\", epsilon)                                                                                                                                                                                                       \n",
    "            data = data.clone()\n",
    "            #print(f\"data: {data}\")\n",
    "            # Set requires_grad attribute of tensor. Important for Attack                                                                                                                                                                    \n",
    "            data.retain_grad()\n",
    "            output = classifier(torch.sum(data, dim=1))\n",
    "            # Calculate gradients of model in backward pass                                                                                                                                                                                  \n",
    "            loss = classifier_loss(output[:,-1], target)\n",
    "            print(f\"Loss: {loss}\")\n",
    "            loss.backward(retain_graph=True)     \n",
    "            data_grad = data.grad.data\n",
    "            #print(f\"data_grad size: {data_grad.size()}\")\n",
    "            #print(f\"data_grad: {data_grad}\")\n",
    "            #print(f\"epsilon * data_grad: {epsilon * data_grad}\")\n",
    "            data = data - epsilon * data_grad\n",
    "            #print(f\"data: {data}\")\n",
    "            epsilon = epsilon * 0.9\n",
    "\n",
    "            encoder_outputs = transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions(\n",
    "                last_hidden_state=data.detach(), \n",
    "                attentions=autoencoder_output.encoder_attentions\n",
    "            )\n",
    "            output = autoencoder.generate(encoder_outputs=encoder_outputs)\n",
    "            print(tokenizer.decode(output[0], skip_special_tokens=True)) \n",
    "\n",
    "\n",
    "\n",
    "autoencoder.eval()\n",
    "classifier.eval()\n",
    "\n",
    "#autoencoder_output = autoencoder(input_ids=tokenizer_output.input_ids, attention_mask=tokenizer_output.attention_mask, labels=tokenizer_output.input_ids, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "#print(f\"Original sentence: {tokenizer.decode(tokenizer_output['input_ids'][0], skip_special_tokens=True)}\")\n",
    "#autoencoder_output = autoencoder(input_ids=tokenizer_output.input_ids, attention_mask=tokenizer_output.attention_mask, labels=tokenizer_output.input_ids, output_attentions=True, output_hidden_states=True)\n",
    "#fast_gradient_iterative_modification(autoencoder, classifier, autoencoder_output, torch.ones(1), tokenizer)\n",
    "\n",
    "for sample in legal_dataset:\n",
    "    input_ids = sample[\"input_ids\"].unsqueeze(0)\n",
    "    print(f\"Original sentence: {tokenizer.decode(input_ids[0], skip_special_tokens=True)}\")\n",
    "    attention_mask = sample[\"attention_mask\"].unsqueeze(0)\n",
    "    autoencoder_output = autoencoder.forward(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids, output_attentions=True, output_hidden_states=True)\n",
    "    fast_gradient_iterative_modification(autoencoder, classifier, autoencoder_output, torch.zeros(NON_LEGAL_TEXT_LABEL), tokenizer)\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ce3489-b382-49e3-882c-628515eb11ce",
   "metadata": {},
   "source": [
    "```python\n",
    "#generated_text = generate_text(tokenizer, autoencoder, data)\n",
    "#print(\"| It {:2d} | classifier model pred {:5.4f} |\".format(it, output[0].item()))                                                                                                                                                      \n",
    "#print(generated_text)\n",
    "\n",
    "\n",
    "def generate_text(tokenizer, decoder, latent_space): \n",
    "    decoder_input_ids = torch.full((latent_space.size(0), 1), tokenizer.cls_token_id )\n",
    "    for position in range(MAX_SEQUENCE_LENGTH):\n",
    "        attention_mask[:, position] = NOT_MASKED\n",
    "        output = decoder(decoder_input_ids, encoder_hidden_states=latent_space)\n",
    "        probabilities = torch.nn.functional.log_softmax(output.logits, dim=-1)\n",
    "        _, next_word = torch.max(probabilities, dim=-1)        \n",
    "        next_word = next_word[:,position]\n",
    "        decoder_input_ids[:, position] = next_word   \n",
    "    text = tokenizer.batch_decode(decoder_input_ids, skip_special_tokens=skip_special_tokens)\n",
    "    return text\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
